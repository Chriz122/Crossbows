# ğŸš€ ç¬¬äºŒéšæ®µæ”¹é€²ï¼šè§£æ±ºå¹…åº¦ä¸è¶³å•é¡Œ

## ğŸ“Š ç•¶å‰å•é¡Œåˆ†æ

æ ¹æ“šæ‚¨æä¾›çš„åœ–è¡¨ï¼š

### âœ… å·²è§£æ±º
- **Yaw ä¸å†æ˜¯å¹³çš„**ï¼šèƒ½è·Ÿéš¨è¶¨å‹¢è®ŠåŒ–
- **æ¨™æº–åŒ–å•é¡Œä¿®å¾©**ï¼šè¼‰å…¥æ­£ç¢ºçš„ scaler

### âš ï¸ ä»å­˜åœ¨çš„å•é¡Œ

#### 1. **Yaw å¹…åº¦ä¸è¶³** (æœ€åš´é‡)
- **çœŸå¯¦å€¼ç¯„åœ**: -50 åˆ° 100 (æŒ¯å¹… ~150)
- **é æ¸¬å€¼ç¯„åœ**: -50 åˆ° 50 (æŒ¯å¹… ~100)
- **å•é¡Œ**: é æ¸¬çš„æŒ¯å¹…ç´„ç‚ºçœŸå¯¦å€¼çš„ **66%**

#### 2. **Roll æ™‚é–“å»¶é²**
- Predict åœ–ä¸­ Roll æ˜é¡¯æ»¯å¾Œæ–¼çœŸå¯¦å€¼
- å­˜åœ¨æŠ–å‹•å’Œä¸ç©©å®š

#### 3. **Pitch å€æ®µåç§»**
- åœ¨ sample 8000+ å‡ºç¾ç³»çµ±æ€§åç§»

---

## ğŸ¯ ç¬¬äºŒéšæ®µæ”¹é€²æ–¹æ¡ˆ

### æ”¹é€² 1: **CNN + Attention æ©Ÿåˆ¶**

#### ç‚ºä»€éº¼éœ€è¦ CNNï¼Ÿ
æ‰‹éƒ¨é—œéµé»åœ¨ç©ºé–“ä¸Šæœ‰çµæ§‹æ€§é—œä¿‚ï¼ˆä¾‹å¦‚ï¼šæ‰‹æŒ‡ä¹‹é–“ï¼‰ã€‚CNN èƒ½æå–é€™äº›å±€éƒ¨æ¨¡å¼ã€‚

```python
# 1D CNN æå–å±€éƒ¨ç‰¹å¾µ
self.conv1d = nn.Sequential(
    nn.Conv1d(in_channels=189, out_channels=256, kernel_size=3, padding=1),
    nn.BatchNorm1d(256),  # ç©©å®šè¨“ç·´
    nn.ReLU(),
    nn.Dropout(0.15)
)
```

#### ç‚ºä»€éº¼éœ€è¦ Attentionï¼Ÿ
ä¸æ˜¯æ‰€æœ‰æ™‚é–“æ­¥éƒ½åŒæ¨£é‡è¦ã€‚Attention è®“æ¨¡å‹é—œæ³¨é—œéµæ™‚åˆ»ã€‚

```python
# Attention æ©Ÿåˆ¶
self.attention = nn.Sequential(
    nn.Linear(lstm_output_size, lstm_output_size // 2),
    nn.Tanh(),
    nn.Linear(lstm_output_size // 2, 1)
)

# è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
attention_weights = torch.softmax(self.attention(lstm_out), dim=1)
context = torch.sum(lstm_out * attention_weights, dim=1)
```

**æ•ˆæœ**:
- âœ… æå–ç©ºé–“ç‰¹å¾µæ¨¡å¼
- âœ… é—œæ³¨é‡è¦æ™‚é–“æ­¥
- âœ… æå‡å°å¤§å¹…åº¦è®ŠåŒ–çš„æ•æ„Ÿåº¦

---

### æ”¹é€² 2: **æ›´æ·±æ›´å¯¬çš„ç¶²çµ¡**

#### åƒæ•¸å°æ¯”

| é …ç›® | ç¬¬ä¸€ç‰ˆ | ç¬¬äºŒç‰ˆ (ç•¶å‰) |
|------|--------|---------------|
| **CNN** | âŒ ç„¡ | âœ… Conv1d(189â†’256) + BatchNorm |
| **Attention** | âŒ ç„¡ | âœ… åŠ æ¬Šæ³¨æ„åŠ›æ©Ÿåˆ¶ |
| **Residual** | âŒ ç„¡ | âœ… æ®˜å·®é€£æ¥ |
| **hidden_size** | 384 | **512** (+33%) |
| **fc_neurons** | [512,256,128,64,3] | **[768,512,256,128,3]** |
| **dropout** | 0.2 | **0.15** (æ›´æ¿€é€²) |
| **å„ªåŒ–å™¨** | Adam | **AdamW** (æ›´å¥½æ­£å‰‡åŒ–) |
| **å­¸ç¿’ç‡** | 0.001 | **0.002** (2å€) |
| **LR Scheduler** | ReduceLROnPlateau | **CosineAnnealingWarmRestarts** |
| **Loss** | SmoothL1Loss | **HuberLoss(delta=1.0)** |
| **epochs** | 2000 | **3000** |
| **patience** | 100 | **150** |

---

### æ”¹é€² 3: **AdamW + é¤˜å¼¦é€€ç«å­¸ç¿’ç‡**

#### AdamW vs Adam
```python
# AdamW: Adam + è§£è€¦æ¬Šé‡è¡°æ¸›
optimizer = optim.AdamW(model.parameters(), lr=0.002, weight_decay=0.01)
```

**å„ªé»**:
- âœ… æ›´å¥½çš„æ³›åŒ–æ€§èƒ½
- âœ… é¿å…éæ“¬åˆåŒæ™‚ä¿æŒå­¸ç¿’èƒ½åŠ›

#### é¤˜å¼¦é€€ç« (Cosine Annealing Warm Restarts)
```python
scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer, T_0=50, T_mult=2, eta_min=1e-6
)
```

**å·¥ä½œåŸç†**:
```
å­¸ç¿’ç‡è®ŠåŒ–:
Epoch 0-50:    0.002 â†’ 0.000001 (é¤˜å¼¦ä¸‹é™)
Epoch 50:      é‡å•Ÿåˆ° 0.002
Epoch 50-150:  0.002 â†’ 0.000001 (æ›´é•·é€±æœŸ)
...
```

**å„ªé»**:
- âœ… é€±æœŸæ€§é‡å•Ÿå¹«åŠ©è·³å‡ºå±€éƒ¨æœ€å„ª
- âœ… é©æ‡‰ä¸åŒéšæ®µçš„å­¸ç¿’éœ€æ±‚
- âœ… æ¯”å›ºå®šå­¸ç¿’ç‡æ›´ç©©å®š

---

### æ”¹é€² 4: **Huber Loss**

```python
criterion = nn.HuberLoss(delta=1.0)
```

#### Huber vs Smooth L1

| Loss | å°èª¤å·® | å¤§èª¤å·® | é©ç”¨å ´æ™¯ |
|------|--------|--------|----------|
| **MSE** | å¹³æ–¹æ‡²ç½° | éåº¦æ‡²ç½° | ç„¡é›¢ç¾¤å€¼ |
| **MAE** | ç·šæ€§æ‡²ç½° | å¤ªå¯¬å®¹ | é­¯æ£’ä½†æ…¢ |
| **Smooth L1** | å¹³æ–¹ (x<1) | ç·šæ€§ (x>1) | ä¸­ç­‰èª¤å·® |
| **Huber** | å¹³æ–¹ (x<Î´) | ç·šæ€§ (x>Î´) | **å¤§èª¤å·®å ´æ™¯** âœ… |

**ç‚ºä»€éº¼é¸ Huber?**
- Yaw çš„å¤§å¹…åº¦æ³¢å‹•æœƒç”¢ç”Ÿå¤§èª¤å·®
- Huber å°å¤§èª¤å·®æ›´å¯¬å®¹ï¼Œé¼“å‹µæ¨¡å‹å¤§è†½é æ¸¬
- Î´=1.0 æ˜¯ç¶“é©—æœ€å„ªå€¼

---

### æ”¹é€² 5: **Residual Connection (æ®˜å·®é€£æ¥)**

```python
for i, layer in enumerate(self.fc_layers):
    identity = out
    out = layer(out)
    
    # ç¶­åº¦åŒ¹é…æ™‚æ·»åŠ æ®˜å·®
    if i > 0 and i < len(self.fc_layers) - 1 and identity.size(-1) == out.size(-1):
        out = out + identity
```

**å„ªé»**:
- âœ… ç·©è§£æ¢¯åº¦æ¶ˆå¤±
- âœ… è®“ç¶²çµ¡æ›´æ·±ï¼ˆ5å±¤å…¨é€£æ¥ï¼‰
- âœ… ä¿ç•™ä½å±¤ç‰¹å¾µ

---

## ğŸ”¬ æŠ€è¡“ç´°ç¯€

### å®Œæ•´çš„å‰å‘å‚³æ’­æµç¨‹

```
è¼¸å…¥ (batch, 12, 189)
    â†“
1D CNN: (189 â†’ 256) + BatchNorm + ReLU + Dropout
    â†“
BiLSTM: (256 â†’ 512*2=1024) 3å±¤é›™å‘
    â†“
Attention: è¨ˆç®—æ¯å€‹æ™‚é–“æ­¥çš„é‡è¦æ€§æ¬Šé‡
    â†“
Context Vector: åŠ æ¬Šæ±‚å’Œ (1024,)
    â†“
FC Layers: 768 â†’ 512 â†’ 256 â†’ 128 â†’ 3
    â†‘_____|  (Residual Connections)
    â†“
è¼¸å‡º (batch, 3)  [Yaw, Pitch, Roll]
```

---

## ğŸ“ˆ é æœŸæ”¹å–„æ•ˆæœ

### Yaw è»¸
- âœ… **å¹…åº¦æ“´å¤§**: å¾ 66% â†’ **90%+** çš„çœŸå¯¦æŒ¯å¹…
- âœ… **å¤§æ³¢å‹•æ•æ‰**: Attention é—œæ³¨å³°å€¼è®ŠåŒ–
- âœ… **æ›´ç©©å®š**: Huber Loss é¼“å‹µå¤§è†½é æ¸¬

### Pitch & Roll
- âœ… **æ¸›å°‘å»¶é²**: CNN æå–ç©ºé–“ç‰¹å¾µåŠ é€ŸéŸ¿æ‡‰
- âœ… **é™ä½æŠ–å‹•**: æ›´å¤§çš„ç¶²çµ¡å®¹é‡å¹³æ»‘é æ¸¬
- âœ… **ä¿®æ­£åç§»**: æ›´é•·è¨“ç·´æ™‚é–“æ‰¾åˆ°æœ€å„ªè§£

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1ï¸âƒ£ é‡æ–°è¨“ç·´ï¼ˆå¿…é ˆï¼çµæ§‹å®Œå…¨ä¸åŒï¼‰

```powershell
cd c:\Users\USER\Desktop\Crossbows
python cnn_lstm/train.py
```

**è¨“ç·´ç‰¹é»**:
- ğŸ• é è¨ˆ **30-60 åˆ†é˜**ï¼ˆæ›´æ·±ç¶²çµ¡ï¼‰
- ğŸ“Š æ¯ 50 epoch è¼¸å‡ºä¸€æ¬¡
- ğŸ”„ å­¸ç¿’ç‡é€±æœŸæ€§é‡å•Ÿ
- ğŸ’¾ è‡ªå‹•ä¿å­˜æœ€ä½³æ¨¡å‹

### 2ï¸âƒ£ ç›£æ§è¨“ç·´éç¨‹

è§€å¯Ÿè¼¸å‡º:
```
epoch:0, train_loss:0.123456, test_loss:0.234567, lr:0.002000
âœ“ æ–°çš„æœ€ä½³æ¨¡å‹! test_loss: 0.234567

epoch:50, train_loss:0.098765, test_loss:0.198765, lr:0.000001
(å­¸ç¿’ç‡é™åˆ°æœ€ä½ï¼Œå³å°‡é‡å•Ÿ)

epoch:51, train_loss:0.097654, test_loss:0.197654, lr:0.002000
(å­¸ç¿’ç‡é‡å•Ÿï¼)
```

### 3ï¸âƒ£ é æ¸¬

```powershell
python cnn_lstm/predict.py
```

---

## âš ï¸ æ½›åœ¨å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

### Q1: GPU è¨˜æ†¶é«”ä¸è¶³ (OOM)

**ç—‡ç‹€**: `RuntimeError: CUDA out of memory`

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ 1: é™ä½ batch_size
batch_size = 128  # æˆ– 64

# æ–¹æ¡ˆ 2: é™ä½æ¨¡å‹å¤§å°
hidden_size = 384  # å¾ 512 é™åˆ° 384
fc_neurons = [512, 256, 128, 64, 3]  # ç¸®å° FC å±¤

# æ–¹æ¡ˆ 3: æ¸›å°‘å±¤æ•¸
num_layers = 2  # å¾ 3 é™åˆ° 2
```

### Q2: è¨“ç·´å¤ªæ…¢

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# é™ä½ç¸½è¨“ç·´é‡
epochs = 2000
patience = 100

# æˆ–å¢åŠ æª¢æŸ¥é–“éš”
if epoch % 100 == 0:  # å¾ 50 æ”¹æˆ 100
```

### Q3: æå¤±éœ‡ç›ª

**ç—‡ç‹€**: train_loss å’Œ test_loss åŠ‡çƒˆæ³¢å‹•

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# é™ä½åˆå§‹å­¸ç¿’ç‡
optimizer = optim.AdamW(model.parameters(), lr=0.001)  # å¾ 0.002 é™åˆ° 0.001

# å¢åŠ  dropout
dropout = 0.2  # å¾ 0.15 å¢åŠ åˆ° 0.2
```

---

## ğŸ“Š æ¨¡å‹å°æ¯”ç¸½çµ

| ç‰¹æ€§ | åŸºç¤ç‰ˆ | ç¬¬ä¸€ç‰ˆä¿®å¾© | ç¬¬äºŒç‰ˆå¢å¼· (ç•¶å‰) |
|------|--------|------------|-------------------|
| **Yaw é æ¸¬** | âŒ å¹³å¦ | âš ï¸ å¹…åº¦ä¸è¶³ | âœ… é è¨ˆ 90%+ |
| **æ¨™æº–åŒ–** | âŒ éŒ¯èª¤ | âœ… æ­£ç¢º | âœ… æ­£ç¢º |
| **CNN** | âŒ | âŒ | âœ… Conv1d |
| **Attention** | âŒ | âŒ | âœ… æ™‚åºæ³¨æ„åŠ› |
| **Residual** | âŒ | âŒ | âœ… æ®˜å·®é€£æ¥ |
| **åƒæ•¸é‡** | ä¸­ | å¤§ | **è¶…å¤§** |
| **è¨“ç·´æ™‚é–“** | 15 åˆ†é˜ | 20-40 åˆ†é˜ | **30-60 åˆ†é˜** |
| **GPU éœ€æ±‚** | ä½ | ä¸­ | **é«˜** |

---

## ğŸ“ é—œéµå­¸ç¿’é»

### 1. **ç‚ºä»€éº¼éœ€è¦é€™éº¼è¤‡é›œçš„æ¨¡å‹ï¼Ÿ**

**Yaw çš„ç‰¹é»**:
- è®ŠåŒ–ç¯„åœå¤§ (-50 åˆ° 100)
- è®ŠåŒ–é€Ÿåº¦å¿«ï¼ˆæ€¥åŠ‡ä¸Šå‡/ä¸‹é™ï¼‰
- éç·šæ€§é—œä¿‚è¤‡é›œ

**ç°¡å–®æ¨¡å‹çš„å•é¡Œ**:
- å‚¾å‘æ–¼é æ¸¬ä¿å®ˆï¼ˆæ¥è¿‘å‡å€¼ï¼‰
- å°å¤§å¹…åº¦è®ŠåŒ–åæ‡‰ä¸è¶³
- é›£ä»¥æ•æ‰çªè®Š

**è¤‡é›œæ¨¡å‹çš„å„ªå‹¢**:
- CNN: æå–ç©ºé–“æ¨¡å¼
- Attention: è­˜åˆ¥é—œéµæ™‚åˆ»
- Residual: ä¿ç•™ç´°ç¯€ä¿¡æ¯
- å¤§å®¹é‡: è¨˜æ†¶è¤‡é›œæ¨¡å¼

### 2. **Attention çš„é‡è¦æ€§**

åœ¨æ‰‹å‹¢è­˜åˆ¥ä¸­:
- æŸäº›é—œéµå¹€ï¼ˆæ‰‹å‹¢è®ŠåŒ–ç¬é–“ï¼‰æœ€é‡è¦
- Attention è‡ªå‹•å­¸ç¿’é€™äº›é—œéµæ™‚åˆ»
- åŠ æ¬Šæ±‚å’Œæ¯”"åªå–æœ€å¾Œä¸€å¹€"æ›´æ™ºèƒ½

### 3. **å­¸ç¿’ç‡èª¿åº¦çš„è—è¡“**

é¤˜å¼¦é€€ç«çš„å„ªå‹¢:
- å‰æœŸï¼šé«˜å­¸ç¿’ç‡å¿«é€Ÿå­¸ç¿’
- ä¸­æœŸï¼šé™ä½å­¸ç¿’ç‡ç²¾ç´°èª¿æ•´
- é‡å•Ÿï¼šè·³å‡ºå±€éƒ¨æœ€å„ªï¼Œæ¢ç´¢æ–°å€åŸŸ

---

## ğŸ“ ä¸‹ä¸€æ­¥å»ºè­°

### å¦‚æœæ•ˆæœé‚„ä¸ç†æƒ³

#### æ–¹å‘ 1: å¢åŠ æ™‚é–“çª—å£
```python
time_step = 20  # å¾ 12 å¢åŠ åˆ° 20
# çµ¦æ¨¡å‹æ›´å¤šæ­·å²ä¿¡æ¯
```

#### æ–¹å‘ 2: æ•¸æ“šå¢å¼·
```python
# æ·»åŠ è¼•å¾®å™ªéŸ³
features_augmented = features + np.random.normal(0, 0.005, features.shape)

# æ™‚é–“æ‰­æ›²
# è¼•å¾®ç¸®æ”¾æ‰‹éƒ¨é—œéµé»
```

#### æ–¹å‘ 3: å¤šä»»å‹™å­¸ç¿’
```python
# åŒæ™‚é æ¸¬ä½ç½®å’Œé€Ÿåº¦
output = model(x)  # (batch, 6)  [Yaw, Pitch, Roll, dYaw, dPitch, dRoll]
```

#### æ–¹å‘ 4: Transformer
```python
# æ›¿æ› LSTM ç‚º Transformer
encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8)
transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)
```

---

## âœ… ç¸½çµ

### æ ¸å¿ƒæ”¹é€²
1. âœ… **CNN**: æå–ç©ºé–“ç‰¹å¾µ
2. âœ… **Attention**: é—œæ³¨é—œéµæ™‚åˆ»
3. âœ… **Residual**: æ·±åº¦ç¶²çµ¡ç©©å®šè¨“ç·´
4. âœ… **Huber Loss**: å°å¤§èª¤å·®æ›´å¯¬å®¹
5. âœ… **AdamW + é¤˜å¼¦é€€ç«**: æ›´å¥½å„ªåŒ–

### é æœŸçµæœ
- Yaw æŒ¯å¹…: 66% â†’ **90%+**
- Roll å»¶é²: æ˜é¡¯ â†’ **è¼•å¾®**
- Pitch åç§»: æœ‰ â†’ **å°‘**

---

**æ›´æ–°æ—¥æœŸ**: 2025å¹´10æœˆ31æ—¥  
**ç‰ˆæœ¬**: v2.0 (Advanced)  
**ç‹€æ…‹**: âœ… æº–å‚™è¨“ç·´
